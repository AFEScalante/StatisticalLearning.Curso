---
title: "Métodos avanzados de regresión (Shrinkage Methods)"
author: "Ángel Escalante"
date: "2/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

<span style = "color: red;">(Antes de empezar a leer, doy por hecho que ya están muy truchas con regresiones lineales múltiples, ajustar parámetros en $\texttt{R}$, interpretarlos y leves conocimientos de validación cruzada. ¡Éntrele!)</span> 

Los algoritmos de regresión lineal (últimos cuadrados o gradiente de descenso), pueden resultar fuertemente inestables o directamente inaplicables cuando el número de variables $p$ es similar o incluso superior al número de observaciones $n$. Una alternativa son los métodos de regresión penalizada. La idea clave es la penalización, se evita el sobreajuste debido al gran número de variables predictoras imponiendo una penalización o término de penalización, que obligará a que alguna
componente del vector de parámetros $\beta = (\beta_{1},..,\beta_{p})'$ sea cero.

## Regresion Rígida (Ridge Regression)

En la regresión lineal tradicional, el método de mínimos cuadrados ordinarios estima $\beta_{1},..,\beta_{p}$ con los valores que minimizan

$$RSS = \sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^{2}$$

La regresión rígida actúa de forma similar, a diferencia de que los parámetros beta son estimados minimizando una cantidad diferente:

$$\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\sum_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\sum_{j=1}^{p}\beta_{j}^{2}$$

Denotamos estos parámetros como $\beta^{R}$, el parámetro $\lambda \ge 0$ es un parámetro de $\textit{arbitrareo}$ que buscamos tunear, comúnmente con técnicas de validación cruzada. El segundo término $\lambda\sum_{j}\beta_{j}^{2}$ es llamado $\textit{término de penalización}$. Cuando $\lambda = 0$ estamos ajustando básicamente una regresión lineal múltiple. Bajo este enfoque, estimamos un vector de parámetros $\hat{\beta_{\lambda}^{R}}$ para cada posible elección de $\lambda$.

Recapitulando (¡demuéstralo si puedes, perro!):

* Si $\lambda \to 0$ entonces $\hat{\beta}^{R}_{\lambda} \to \hat{\beta}^{OLS}$
* Si $\lambda \to \infty$ entonces $\hat{\beta}^{R}_{\lambda} \to 0$

### Especificación del modelo

La función de pérdida para la regresión rígida se define como

$$ L_{R}(\hat{\beta}) = \sum_{i=1}^{n} (y_{i} - \hat{\beta}_{0} - \sum_{j=1}^{p}\hat{\beta}_{j}x_{ij})^{2}+\lambda\sum_{j=1}^{p}\hat{\beta}_{j}^{2} = ||y-X\hat{\beta}||^{2} + \lambda||\hat{\beta}||^{2}$$

Resolviendo para $\hat{\beta}$ lleva a el estimado $\hat{\beta}^{R}_{\lambda} = (X'X+\lambda I)^{-1}(X'Y)$ (usando notación matricial).

### Trade-off entre Sesgo y Varianza

Incorporaremos el parámetro $\lambda$ a las funciones de Sesgo y Varianza. Si no recuerdas cómo están definidas estas funciones, sólo hay que recordar el caso de regresión lineal múltiple:  $Bias(\hat{\beta})=E[\hat{\beta}]-\beta$ y $Var(\hat{\beta}) = \sigma^{2}(X'X)^{-1}$, dónde podemos remplazar $\sigma^{2}$ por $\hat{\sigma}^{2}=\dfrac{e'e}{n-m}$ y $e=y-X\hat{\beta}$. Entonces, incorporando el coeficiente de regularización $\lambda$ tenemos:

$$ Bias(\hat{\beta}^{R}_{\lambda})=-\lambda(X'X + \lambda I)^{-1}\beta $$
$$ Var(\hat{\beta}^{R}_{\lambda})=\sigma^{2}(X'X + \lambda I)^{-1}X'X(X'X + \lambda I)^{-1} $$

Podemos notar que, entre mayor sea el parámetro de regularización la varianza disminuye, ¡pero el sesgo aumenta!. Esto nos lleva a plantearnos, ¿cuánto sesgo estamos dispuestos a aceptar para disminuir la varianza?, o bien, ¿cuál es el valor óptimo de $\lambda$?

Para esto contamos con dos criterios, el criterio de validación cruzada (que ya mencioné anteriormente) y el de minimización de información (Akaike Information Criteria y Bayesian Information Criteria, ver: https://es.wikipedia.org/wiki/Criterio_de_informaci%C3%B3n_de_Akaike y https://en.wikipedia.org/wiki/Bayesian_information_criterion). En este documento desarrollaré el primero.

```{r cars, message=FALSE, warning=FALSE}
# Cargando paquetes, lectura de datos y sembrando semilla aleatoria para reproducción
set.seed(69)    # sembrar semillas con número mágico para reproducción
library(glmnet)
library(dplyr)
library(psych)

data("mtcars")
# Centrar y, X será estandarizado en las funciones del modelado
y <- mtcars %>% select(mpg) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()
X <- mtcars %>% select(-mpg) %>% as.matrix()
```

Ejecutar validación cruzada y generar gráfica de resultados:

```{r plot1, message=FALSE}
# Ejecutar validación cruzada 10-fold para seleccionar lambda
lambdas_to_try <- 10 ^ seq(-3, 5, length.out = 100)
# alpha = 0 implementar regresión rígida
ridge_cv <- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
# Graficar resultados
plot(ridge_cv)
```

